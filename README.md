**RAG-Based Q&A System**
======================================

ğŸ“ŒÂ **Project Overview**
-----------------------

This project implements aÂ **Retrieval-Augmented Generation (RAG) system**Â for answering questions aboutÂ **Dr. B.R. Ambedkar's speech on the "Annihilation of Caste"**. The system usesÂ **LangChain, ChromaDB, and Ollama**Â to provide accurate, context-aware answers based solely on the provided text.

All components runÂ **100% locally**Â withÂ **no API keys or cloud dependencies**Â required.

ğŸ¯Â **Key Features**
-------------------

âœ”ï¸Â **Document Loading & Text Chunking:**Â Intelligent splitting of text into semantic chunks.

âœ”ï¸Â **Semantic Search:**Â Uses HuggingFace embeddings for accurate retrieval.

âœ”ï¸Â **Context-Aware Q&A:**Â Powered by Ollama Mistral 7B LLM.

âœ”ï¸Â **Persistent Vector Storage:**Â ChromaDB stores embeddings locally (no re-embedding needed).

âœ”ï¸Â **Interactive CLI:**Â Ask questions in real-time or single-question mode.

âœ”ï¸Â **Hallucination Prevention:**Â Refuses to answer out-of-domain questions.

ğŸ—Â **Project Structure**
------------------------

```   Task/  
â”‚  
â”œâ”€â”€ ğŸ“‚ src/  
â”‚   â”œâ”€â”€ ğŸ“œ __init__.py  
â”‚   â”œâ”€â”€ ğŸ“œ document_loader.py       # Load text documents  
â”‚   â”œâ”€â”€ ğŸ“œ text_processor.py        # Text chunking logic  
â”‚   â”œâ”€â”€ ğŸ“œ embeddings_handler.py    # HuggingFace embeddings  
â”‚   â”œâ”€â”€ ğŸ“œ vector_store.py          # ChromaDB operations  
â”‚   â””â”€â”€ ğŸ“œ qa_system.py             # RAG chain & LLM integration  
â”‚   
â”œâ”€â”€ ğŸ“‚ data/  
â”‚   â””â”€â”€ ğŸ“œ speech.txt               # Dr. Ambedkar's speech  
â”‚  
â”œâ”€â”€ ğŸ“‚ chroma_db/                   # Vector database (auto-created)  
â”œâ”€â”€ ğŸ“œ main.py                      # CLI entry point  
â”œâ”€â”€ ğŸ“œ pyproject.toml               # Dependencies  
â”œâ”€â”€ ğŸ“œ README.md                    # Project documentation  
â””â”€â”€ ğŸ“œ .gitignore   
```

âš™ï¸Â **Technical Stack**
----------------------
**Orchestration:** LangChain 

**Vector Database:** ChromaDB (persistent, local)

**Embeddings:** sentence-transformers/all-MiniLM-L6-v2 (384 dims)

**LLM:** Ollama Mistral 7B (temperature=0.2)

**Chunk Size:** 500 characters (50 overlap)

**Retrieval:** Top-3 similarity search (cosine)

ğŸ› ï¸Â **Prerequisites**
---------------------

Before running this project, ensure you have:

1ï¸âƒ£Â **Python 3.10+**Â installed

2ï¸âƒ£Â **uv package manager**Â ([Installation Guide](https://github.com/astral-sh/uv))

3ï¸âƒ£Â **Ollama with Mistral model**Â ([Download Ollama](https://ollama.com/download))

**Install Ollama and Mistral:**

```   # Download and install Ollama from https://ollama.com/download  # Then pull the Mistral model:  ollama pull mistral   ```

ğŸ“¦Â **Installation**
-------------------

1ï¸âƒ£Â **Clone the Repository:**

```   git clone https://github.com/HimanshuBhosale25/RAG-Task.git  cd Task   ```

2ï¸âƒ£Â **Initialize uv for project:**

```   uv init   ```

3ï¸âƒ£Â **Install Dependencies:**

```   uv sync   ```

**Note:**Â First run will download the sentence-transformers model (~120MB), which takes 2-5 minutes.

ğŸš€Â **How to Use**
-----------------

**Interactive Mode (Recommended):**

```   uv run main.py   ```


**Single Question Mode:**

```   uv run main.py -q "What is the real enemy?"   ```

**Force Rebuild Vector Store:**

```   uv run main.py --rebuild   ```

**Custom Speech File:**

```   uv run main.py --speech-file "path/to/custom/speech.txt"   ```

ğŸ“Š **Sample Output**

![Terminal](images/i1.png)

ğŸ§ªÂ **Example Questions**
------------------------

**Direct Questions:**

âœ… "What is the real remedy according to the text?"

âœ… "What cannot exist together?"

âœ… "What metaphor describes social reform?"

**Paraphrased Questions:**

âœ… "Why won't people get rid of caste?"

âœ… "Is caste a social reform problem?"

**Out-of-Domain (Should refuse):**

âŒ "What was Dr. Ambedkar's birthdate?"

âŒ "Tell me about Python programming."

ğŸ“ŒÂ **Methodology & Design Choices**
-----------------------------------

1ï¸âƒ£Â **Document Loading**Â -- Used LangChain's TextLoader to read speech.txt.

2ï¸âƒ£Â **Text Chunking**Â -- Split into 500-character chunks with 50-char overlap for context preservation.

3ï¸âƒ£Â **Embeddings**Â -- HuggingFace all-MiniLM-L6-v2 model withÂ **normalized vectors**Â for accurate similarity.

4ï¸âƒ£Â **Vector Storage**Â -- ChromaDB persistent storage prevents re-embedding on every run.

5ï¸âƒ£Â **Retrieval**Â -- Top-3 similar chunks retrieved using cosine similarity.

6ï¸âƒ£Â **Generation**Â -- Ollama Mistral 7B withÂ **low temperature (0.2)**Â to reduce hallucinations.

7ï¸âƒ£Â **Prompt Engineering**Â -- Explicit instructions to answer only from context, refuse out-of-domain questions.

**Why These Choices?**

**Chunk size 500 chars:**Â Optimal for short document (~600 chars total)

**Temperature 0.2:**Â Minimizes hallucinations for factual Q&A

**k=3 retrieval:**Â Comprehensive context for small corpus

**Normalized embeddings:**Â Improves cosine similarity accuracy


ğŸ“Â **Dependencies**
-------------------

Core packages (seeÂ pyproject.tomlÂ for full list):

`
langchain>=1.0.7
langchain-community>=0.4.1
langchain-ollama>=1.0.0
langchain-huggingface>=0.1.0
langchain-chroma>=0.2.0
langchain-classic>=1.0.0
chromadb>=1.3.4
sentence-transformers>=5.1.2
`

ğŸ“‚Â **Deliverables**
-------------------

âœ…Â **Python Source Code**Â -- Modular, well-commented implementation inÂ src/

âœ…Â **Main Entry Point**Â --Â main.pyÂ with CLI interface

âœ…Â **Dependencies File**Â --Â pyproject.tomlÂ with uv package manager

âœ…Â **Documentation**Â -- This README with setup and usage instructions

âœ…Â **Source Text**Â --Â data/speech.txtÂ with Dr. Ambedkar's speech

ğŸ†Â **Conclusion**
-----------------

This project successfully demonstrates aÂ **clean and functional RAG pipeline**Â using modern LangChain architecture. The system balancesÂ **accuracy, efficiency, and risk mitigation**Â (hallucination prevention) while runningÂ **100% locally**Â with no external API dependencies.
